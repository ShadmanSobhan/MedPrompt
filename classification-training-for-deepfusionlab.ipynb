{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12196859,"sourceType":"datasetVersion","datasetId":7604236}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport warnings\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchinfo import summary\n\nfrom torchvision.models.efficientnet import efficientnet_b0, EfficientNet_B0_Weights\n\nimport albumentations as album\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nwarnings.filterwarnings(\"ignore\")\n\n# Set device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n)\n\nimport time\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n)\nimport numpy as np\n\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source_base = \"/kaggle/input/combined-dataset-pt1/C3_ShenzhenMontgomery_Classification_TB\"\nEPOCHS = 100\npat = 15","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom PIL import Image\nfrom pathlib import Path\n\n# Define source and destination base paths\ndest_base = \"/kaggle/working/data-resized\"\n\n# Define subdirectories\nsubdirs = [\n    \"test/Normal\",\n    \"test/Tuberculosis\",\n    \"train/Normal\",\n    \"train/Tuberculosis\"\n]\n\n# Desired image size (e.g., 256x256)\ntarget_size = (256, 256)\n\n# Create the directory structure\nfor subdir in subdirs:\n    os.makedirs(os.path.join(dest_base, subdir), exist_ok=True)\n\n# Function to resize and save image\ndef resize_and_save_image(src_path, dst_path, size):\n    with Image.open(src_path) as img:\n        img = img.resize(size, Image.LANCZOS)\n        img.save(dst_path)\n\n# Process each folder\nfor subdir in subdirs:\n    src_folder = os.path.join(source_base, subdir)\n    dst_folder = os.path.join(dest_base, subdir)\n\n    for file_name in os.listdir(src_folder):\n        src_file = os.path.join(src_folder, file_name)\n        dst_file = os.path.join(dst_folder, file_name)\n\n        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n            resize_and_save_image(src_file, dst_file, target_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_image_file(filename):\n    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n\ndef explore_directory(base_path):\n    print(f\"üìÇ Exploring: {base_path}\\n\")\n\n    for root, dirs, files in os.walk(base_path):\n        level = root.replace(base_path, '').count(os.sep)\n        indent = '  ' * level\n        sub_indent = '  ' * (level + 1)\n        \n        print(f\"{indent}üìÅ {os.path.basename(root)}/\")\n        \n        image_files = [f for f in files if is_image_file(f)]\n        other_files = [f for f in files if not is_image_file(f)]\n        \n        if image_files:\n            print(f\"{sub_indent}üñºÔ∏è  Image files: {len(image_files)}\")\n        if other_files:\n            print(f\"{sub_indent}üìÑ Other files: {len(other_files)}\")\n        if not image_files and not other_files:\n            print(f\"{sub_indent}(Empty folder)\")\n\n# Run the function\nexplore_directory('/kaggle/working/data-resized')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Define","metadata":{}},{"cell_type":"code","source":"# SeparableConv2d remains unchanged\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):\n        super().__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        return self.pointwise(self.depthwise(x))\n\n# ASPP remains unchanged\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, out_channels, atrous_rates):\n        super().__init__()\n        modules = [\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n        ]\n        for rate in atrous_rates:\n            modules.append(nn.Sequential(\n                SeparableConv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            ))\n        modules.append(nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ))\n        self.convs = nn.ModuleList(modules)\n        self.project = nn.Sequential(\n            nn.Conv2d((len(atrous_rates) + 2) * out_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.5)\n        )\n\n    def forward(self, x):\n        size = x.shape[2:]\n        res = [F.interpolate(conv(x), size=size, mode='bilinear', align_corners=True) if i == len(self.convs)-1 else conv(x) for i, conv in enumerate(self.convs)]\n        return self.project(torch.cat(res, dim=1))\n\n# MFF Block\nclass MFFBlock(nn.Module):\n    def __init__(self, in_channels_low, in_channels_high, out_channels):\n        super().__init__()\n        self.low_proj = nn.Conv2d(in_channels_low, out_channels, 1, bias=False)\n        self.high_proj = nn.Conv2d(in_channels_high, out_channels, 1, bias=False)\n        self.fusion = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(out_channels, out_channels // 8, 1),\n            nn.ReLU(),\n            nn.Conv2d(out_channels // 8, out_channels, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, low_feat, high_feat):\n        high_feat = F.interpolate(high_feat, size=low_feat.shape[2:], mode='bilinear', align_corners=True)\n        low_feat = self.low_proj(low_feat)\n        high_feat = self.high_proj(high_feat)\n        x = low_feat + high_feat\n        x = self.fusion(x)\n        return x * self.se(x)\n\n# CAFSE Block\nclass CAFSEBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.coarse = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True)\n        )\n        self.fine = nn.Sequential(\n            nn.Conv2d(channels, channels, 1),\n            nn.BatchNorm2d(channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, decoder_feat, aspp_feat):\n        aspp_feat = F.interpolate(aspp_feat, size=decoder_feat.shape[2:], mode='bilinear', align_corners=True)\n        coarse = self.coarse(aspp_feat)\n        fine = self.fine(decoder_feat)\n        return decoder_feat + coarse * fine\n\n# Decoder remains unchanged\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, 48, 1, bias=False),\n            nn.BatchNorm2d(48),\n            nn.ReLU(inplace=True)\n        )\n        self.fuse = nn.Sequential(\n            SeparableConv2d(96, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            SeparableConv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.3)\n        )\n\n    def forward(self, x, low_level_feat):\n        x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=True)\n        x = self.conv1(x)\n        x = torch.cat([x, low_level_feat], dim=1)\n        return self.fuse(x)\n\n# Main model\nclass DeepFusionLab(nn.Module):\n    def __init__(self, num_classes_seg=1, num_classes_cls=2, mode=1, output_stride=16, activation='sigmoid'):\n        super().__init__()\n        self.mode = mode\n        self.output_stride = output_stride\n\n        backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n        features = list(backbone.features.children())\n        if output_stride == 16:\n            self.low_level = nn.Sequential(*features[:3])\n            self.high_level = nn.Sequential(*features[3:])\n        else:\n            self.low_level = nn.Sequential(*features[:2])\n            self.high_level = nn.Sequential(*features[2:])\n\n        low_level_channels = 24 if output_stride == 16 else 16\n        self.low_proj = nn.Sequential(\n            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n            nn.BatchNorm2d(48),\n            nn.ReLU(inplace=True)\n        )\n\n        atrous_rates = [6, 12, 18] if output_stride == 16 else [12, 24, 36]\n        self.aspp = ASPP(1280, 256, atrous_rates)\n        self.mff = MFFBlock(48, 256, 256)\n        self.decoder = Decoder(256, 256)\n        self.cafse = CAFSEBlock(256)\n        self.final_conv = nn.Conv2d(256, num_classes_seg, 1)\n\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes_cls)\n        )\n\n        if activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        elif activation == 'softmax2d':\n            self.activation = nn.Softmax2d()\n        else:\n            self.activation = None\n\n    def forward(self, x):\n        input_size = x.size()[2:]\n        low_feat = self.low_level(x)\n        high_feat = self.high_level(low_feat)\n        low_proj = self.low_proj(low_feat)\n\n        if self.mode == 0:\n            out = self.classifier(high_feat)\n            return out\n        elif self.mode == 1:\n            aspp_out = self.aspp(high_feat)\n            mff_out = self.mff(low_proj, aspp_out)\n            decoder_out = self.decoder(mff_out, low_proj)\n            cafse_out = self.cafse(decoder_out, aspp_out)\n            out = self.final_conv(cafse_out)\n            out = F.interpolate(out, size=input_size, mode='bilinear', align_corners=True)\n            if self.activation is not None:\n                out = self.activation(out)\n            return out\n        else:\n            raise ValueError(\"Mode must be 0 (classification) or 1 (segmentation)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = DeepFusionLab(num_classes_seg=2, num_classes_cls=2, mode=0)  # mode=1 for segmentation\nmodel = model.to(DEVICE)\n\n# Input image\ninput_tensor = torch.randn(2, 3, 256, 256).to(DEVICE)\n\n# Forward pass\noutput = model(input_tensor)\nprint(output.shape)\n\ndef print_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    non_trainable_params = total_params - trainable_params\n\n    print(f\"Total Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\")\n    print(f\"Non-Trainable Parameters: {non_trainable_params:,}\")\n\n# Example usage\nprint_model_parameters(model)\n\nsummary(model, input_size=(2, 3, 256, 256))","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataframe","metadata":{}},{"cell_type":"code","source":"# Define paths\ndata_dir_train = \"/kaggle/working/data-resized/train\"\ndata_dir_test = \"/kaggle/working/data-resized/test\"\n\n# Load filepaths and labels\nfilepaths, labels = [], []\n\nfor fold in os.listdir(data_dir_train):\n    foldpath = os.path.join(data_dir_train, fold)\n    for file in os.listdir(foldpath):\n        filepaths.append(os.path.join(foldpath, file))\n        labels.append(fold)\n\n# Create full training dataframe\ntrain_df_full = pd.DataFrame({\"image_path\": filepaths, \"label\": labels})\n\n# Split into train and validation sets\ntrain_df, valid_df = train_test_split(train_df_full, test_size=0.2, random_state=42, stratify=train_df_full['label'])\n\n# Load test data\ntest_filepaths, test_labels = [], []\nfor fold in os.listdir(data_dir_test):\n    foldpath = os.path.join(data_dir_test, fold)\n    for file in os.listdir(foldpath):\n        test_filepaths.append(os.path.join(foldpath, file))\n        test_labels.append(fold)\n\ntest_df = pd.DataFrame({\"image_path\": test_filepaths, \"label\": test_labels})\n\n\n# Map class names to indices\nclass_to_idx = {cls: idx for idx, cls in enumerate(sorted(train_df['label'].unique()))}\nprint(\"Class to index mapping:\", class_to_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Dataset Class\nclass MyDataGenerator(Dataset):\n    def __init__(self, df, class_to_idx, augmentation=None, preprocessing=None):\n        self.image_paths = df['image_path'].tolist()\n        self.labels = df['label'].map(class_to_idx).tolist()\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, i):\n        image = cv2.imread(self.image_paths[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = self.labels[i]\n\n        if self.augmentation:\n            image = self.augmentation(image=image)['image']\n\n        if self.preprocessing:\n            image = self.preprocessing(image=image)['image']\n\n        return image, label\n\n\n# Augmentations\ndef get_training_augmentation():\n    return album.Compose([\n        album.HorizontalFlip(p=0.5),\n        album.VerticalFlip(p=0.5),\n        album.RandomRotate90(p=0.5),\n        album.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n        album.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n    ])\n\ndef get_validation_augmentation():\n    return album.Compose([\n        album.PadIfNeeded(min_height=256, min_width=256, always_apply=True,\n                           border_mode=cv2.BORDER_CONSTANT, value=0)\n    ])\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing():\n    return album.Compose([\n        album.Resize(height=256, width=256, always_apply=True),\n        album.Lambda(image=to_tensor)\n    ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Compute Metrics Function ---\ndef compute_metrics(outputs, labels, calc_auc=False):\n    _, preds = torch.max(outputs, 1)\n    labels_cpu = labels.cpu()\n    preds_cpu = preds.cpu()\n\n    acc = accuracy_score(labels_cpu, preds_cpu)\n    prec = precision_score(labels_cpu, preds_cpu, average='macro')\n    rec = recall_score(labels_cpu, preds_cpu, average='macro')\n    f1 = f1_score(labels_cpu, preds_cpu, average='macro')\n\n    auc = float('nan')\n    if calc_auc:\n        probs = F.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()\n        labels_np = labels_cpu.numpy()\n        try:\n            auc = roc_auc_score(labels_np, probs)\n        except ValueError:\n            pass\n\n    return acc, prec, rec, f1, auc\n\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.001, metric='val_loss'):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.metric = metric\n        self.best_score = None\n        self.wait = 0\n        self.stop_training = False\n\n    def __call__(self, metrics_dict):\n        raw_score = metrics_dict[self.metric]\n        \n        # Determine if this metric should be minimized (e.g., val_loss) or maximized (e.g., accuracy)\n        minimize = self.metric == 'val_loss'\n        current_score = -raw_score if minimize else raw_score\n\n        # Initialize best score if first epoch\n        if self.best_score is None:\n            self.best_score = current_score\n            self.wait = 0\n        elif current_score < self.best_score + self.min_delta:\n            self.wait += 1\n            print(f\"No improvement in {self.metric}. Wait: {self.wait}/{self.patience}\")\n            if self.wait >= self.patience:\n                self.stop_training = True\n                print(\"Early stopping triggered!\")\n        else:\n            self.best_score = current_score\n            self.wait = 0\n\n        # Debug print in original metric scale\n        best_raw = -self.best_score if minimize else self.best_score\n        print(f\"[DEBUG] {self.metric}: Current={raw_score:.6f}, Best={best_raw:.6f}\")\n\n# Dataloaders\ntrain_dataset = MyDataGenerator(train_df, class_to_idx, get_training_augmentation(), get_preprocessing())\nvalid_dataset = MyDataGenerator(valid_df, class_to_idx, get_validation_augmentation(), get_preprocessing())\ntest_dataset = MyDataGenerator(test_df, class_to_idx, get_validation_augmentation(), get_preprocessing())\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n\n# Loss, Optimizer, Scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=1e-9)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# --- Initialize Metric Trackers ---\ntrain_losses, valid_losses = [], []\ntrain_accs, valid_accs = [], []\ntrain_precs, valid_precs = [], []\ntrain_recs, valid_recs = [], []\ntrain_f1s, valid_f1s = [], []\n\nbest_acc = 0.0\nearly_stopping = EarlyStopping(patience=pat, min_delta=1e-20, metric='val_loss')\n\nstart_time = time.time()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0.0\n    train_acc = train_prec = train_rec = train_f1 = 0.0\n\n    for images, labels in train_loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE).long()\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        acc, prec, rec, f1, _ = compute_metrics(outputs, labels, calc_auc=False)\n        train_loss += loss.item() * images.size(0)\n        train_acc += acc * images.size(0)\n        train_prec += prec * images.size(0)\n        train_rec += rec * images.size(0)\n        train_f1 += f1 * images.size(0)\n\n    train_loss /= len(train_loader.dataset)\n    train_acc /= len(train_loader.dataset)\n    train_prec /= len(train_loader.dataset)\n    train_rec /= len(train_loader.dataset)\n    train_f1 /= len(train_loader.dataset)\n\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    train_precs.append(train_prec)\n    train_recs.append(train_rec)\n    train_f1s.append(train_f1)\n\n    model.eval()\n    valid_loss = 0.0\n    valid_acc = valid_prec = valid_rec = valid_f1 = 0.0\n\n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE).long()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            acc, prec, rec, f1, _ = compute_metrics(outputs, labels, calc_auc=False)\n            valid_loss += loss.item() * images.size(0)\n            valid_acc += acc * images.size(0)\n            valid_prec += prec * images.size(0)\n            valid_rec += rec * images.size(0)\n            valid_f1 += f1 * images.size(0)\n\n    valid_loss /= len(valid_loader.dataset)\n    valid_acc /= len(valid_loader.dataset)\n    valid_prec /= len(valid_loader.dataset)\n    valid_rec /= len(valid_loader.dataset)\n    valid_f1 /= len(valid_loader.dataset)\n\n    valid_losses.append(valid_loss)\n    valid_accs.append(valid_acc)\n    valid_precs.append(valid_prec)\n    valid_recs.append(valid_rec)\n    valid_f1s.append(valid_f1)\n\n    print(f\"Epoch {epoch}: \"\n          f\"Train Loss={train_loss:.4f}, Acc={train_acc:.4f}, Prec={train_prec:.4f}, Rec={train_rec:.4f}, F1={train_f1:.4f} | \"\n          f\"Valid Loss={valid_loss:.4f}, Acc={valid_acc:.4f}, Prec={valid_prec:.4f}, Rec={valid_rec:.4f}, F1={valid_f1:.4f}\")\n\n    if valid_acc > best_acc:\n        best_acc = valid_acc\n        torch.save(model.state_dict(), 'Best_Weight.pth')\n        print(\"Model saved!\")\n\n    metrics_dict = {'val_loss': valid_loss}\n    early_stopping(metrics_dict)\n    if early_stopping.stop_training:\n        print(f\"Training stopped at epoch {epoch}\")\n        break\n\n    scheduler.step()\n\n# --- Print Total Time ---\nend_time = time.time()\nelapsed = end_time - start_time\nprint(f\"\\nTotal training time: {elapsed/60:.2f} minutes\")\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# --- Subplots for Loss and Accuracy ---\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))  # 2 rows, 1 column\n\n# Loss\naxes[0].plot(train_losses, label='Train')\naxes[0].plot(valid_losses, label='Validation')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Loss per Epoch')\naxes[0].legend()\naxes[0].grid(True)\naxes[0].set_ylim(0, 1)  # optional, keep y-axis consistent\n\n# Accuracy\naxes[1].plot(train_accs, label='Train')\naxes[1].plot(valid_accs, label='Validation')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Accuracy per Epoch')\naxes[1].legend()\naxes[1].grid(True)\naxes[1].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# --------------------------\n# Load model\n# --------------------------\nmodel.load_state_dict(torch.load('Best_Weight.pth'))\nmodel.eval()\n\n# --------------------------\n# Compute predictions\n# --------------------------\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(DEVICE)\n        outputs = model(images)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.numpy())\n\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\n\n# --------------------------\n# Classification report\n# --------------------------\nreport = classification_report(\n    all_labels, all_preds,\n    target_names=list(class_to_idx.keys()),\n    output_dict=True\n)\n\n# --- Print header ---\nprint(f\"{'Class':15} {'Precision':>10} {'Recall':>10} {'F1-score':>10} {'Support':>10}\")\n\n# --- Print class rows ---\nfor cls in list(class_to_idx.keys()):\n    metrics = report[cls]\n    print(f\"{cls:15} \"\n          f\"{metrics['precision']:10.4f} \"\n          f\"{metrics['recall']:10.4f} \"\n          f\"{metrics['f1-score']:10.4f} \"\n          f\"{metrics['support']:10.0f}\")\n\n# --- Print summary rows ---\nprint(\"\\nOverall Metrics:\")\nfor key in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n    if key == \"accuracy\":\n        print(f\"{'Accuracy':15} {report[key]:10.4f}\")\n    else:\n        metrics = report[key]\n        print(f\"{key:15} \"\n              f\"{metrics['precision']:10.4f} \"\n              f\"{metrics['recall']:10.4f} \"\n              f\"{metrics['f1-score']:10.4f} \"\n              f\"{metrics['support']:10.0f}\")\n\n# --------------------------\n# Confusion matrix\n# --------------------------\ncm = confusion_matrix(all_labels, all_preds)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nclass_names = list(class_to_idx.keys())\n\n# Create annotations with \"count (percent%)\"\nannot = np.empty_like(cm).astype(str)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        annot[i, j] = f\"{cm[i, j]} ({cm_normalized[i, j]*100:.2f}%)\"\n\n# Plot combined confusion matrix\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm_normalized, annot=annot, fmt='', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix (Count and Percentage)')\nplt.show()\n\n# --------------------------\n# Misclassified indices\n# --------------------------\nmisclassified_indices = np.where(all_preds != all_labels)[0]\nprint(\"Misclassified indices:\", misclassified_indices)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}