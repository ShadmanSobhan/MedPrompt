{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12196859,"sourceType":"datasetVersion","datasetId":7604236}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Standard libraries\nimport os\nimport shutil\nimport random\nfrom pathlib import Path\nimport warnings\nimport time\n\nwarnings.filterwarnings(\"ignore\")\n\n# Image processing\nimport cv2\nfrom PIL import Image\n\n# Data handling\nimport numpy as np\nimport pandas as pd\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# Torchvision\nfrom torchvision.models.efficientnet import efficientnet_b0, EfficientNet_B0_Weights\n\n# Albumentations\nimport albumentations as album\nfrom albumentations.pytorch import ToTensorV2\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Set device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Processing","metadata":{}},{"cell_type":"code","source":"src_base = \"/kaggle/input/combined-dataset-pt1/S5_STARE_Dataset\"\nEPOCHS = 200\npat = 50 #Patience for Early stopping","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\n# --------------------------\n# PARAMETERS\n# --------------------------\nworking_base = \"/kaggle/working\"                                  # working directory\nresized_base = os.path.join(working_base, \"Resized\")             # output resized folder\ntarget_size = (256, 256)                                         # desired size\n\n# --------------------------\n# FUNCTIONS\n# --------------------------\ndef is_image_file(filename):\n    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n\ndef resize_image(src_path, dst_path, is_mask=False, size=(256, 256)):\n    with Image.open(src_path) as img:\n        if is_mask:\n            img = img.resize(size, Image.NEAREST)  # preserve mask labels\n        else:\n            img = img.resize(size, Image.LANCZOS)  # high-quality resize for images\n        img.save(dst_path)\n\ndef process_dataset(src_base, dst_base):\n    \"\"\"\n    Automatically finds all subfolders in src_base, treats folders containing\n    'mask' or 'groundtruth' as masks, others as images.\n    \"\"\"\n    for root, dirs, files in os.walk(src_base):\n        if not files:\n            continue\n\n        # Compute destination folder\n        rel_path = os.path.relpath(root, src_base)\n        dst_folder = os.path.join(dst_base, rel_path)\n        os.makedirs(dst_folder, exist_ok=True)\n\n        # Determine if folder is mask folder\n        is_mask = any(x in root.lower() for x in ['mask', 'groundtruth'])\n\n        # Process all image files\n        for f in files:\n            if not is_image_file(f):\n                continue\n            src_path = os.path.join(root, f)\n            dst_path = os.path.join(dst_folder, f)\n            resize_image(src_path, dst_path, is_mask=is_mask, size=target_size)\n\n# --------------------------\n# EXECUTION\n# --------------------------\nprocess_dataset(src_base, resized_base)\nprint(\"Dataset processed and resized successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Define","metadata":{}},{"cell_type":"code","source":"# SeparableConv2d remains unchanged\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):\n        super().__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        return self.pointwise(self.depthwise(x))\n\n# ASPP remains unchanged\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, out_channels, atrous_rates):\n        super().__init__()\n        modules = [\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n        ]\n        for rate in atrous_rates:\n            modules.append(nn.Sequential(\n                SeparableConv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            ))\n        modules.append(nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ))\n        self.convs = nn.ModuleList(modules)\n        self.project = nn.Sequential(\n            nn.Conv2d((len(atrous_rates) + 2) * out_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.5)\n        )\n\n    def forward(self, x):\n        size = x.shape[2:]\n        res = [F.interpolate(conv(x), size=size, mode='bilinear', align_corners=True) if i == len(self.convs)-1 else conv(x) for i, conv in enumerate(self.convs)]\n        return self.project(torch.cat(res, dim=1))\n\n# MFF Block\nclass MFFBlock(nn.Module):\n    def __init__(self, in_channels_low, in_channels_high, out_channels):\n        super().__init__()\n        self.low_proj = nn.Conv2d(in_channels_low, out_channels, 1, bias=False)\n        self.high_proj = nn.Conv2d(in_channels_high, out_channels, 1, bias=False)\n        self.fusion = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(out_channels, out_channels // 8, 1),\n            nn.ReLU(),\n            nn.Conv2d(out_channels // 8, out_channels, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, low_feat, high_feat):\n        high_feat = F.interpolate(high_feat, size=low_feat.shape[2:], mode='bilinear', align_corners=True)\n        low_feat = self.low_proj(low_feat)\n        high_feat = self.high_proj(high_feat)\n        x = low_feat + high_feat\n        x = self.fusion(x)\n        return x * self.se(x)\n\n# CAFSE Block\nclass CAFSEBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.coarse = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True)\n        )\n        self.fine = nn.Sequential(\n            nn.Conv2d(channels, channels, 1),\n            nn.BatchNorm2d(channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, decoder_feat, aspp_feat):\n        aspp_feat = F.interpolate(aspp_feat, size=decoder_feat.shape[2:], mode='bilinear', align_corners=True)\n        coarse = self.coarse(aspp_feat)\n        fine = self.fine(decoder_feat)\n        return decoder_feat + coarse * fine\n\n# Decoder remains unchanged\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, 48, 1, bias=False),\n            nn.BatchNorm2d(48),\n            nn.ReLU(inplace=True)\n        )\n        self.fuse = nn.Sequential(\n            SeparableConv2d(96, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            SeparableConv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.3)\n        )\n\n    def forward(self, x, low_level_feat):\n        x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=True)\n        x = self.conv1(x)\n        x = torch.cat([x, low_level_feat], dim=1)\n        return self.fuse(x)\n\n# Main model\nclass DeepFusionLab(nn.Module):\n    def __init__(self, num_classes_seg=1, num_classes_cls=2, mode=1, output_stride=16, activation='sigmoid'):\n        super().__init__()\n        self.mode = mode\n        self.output_stride = output_stride\n\n        backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n        features = list(backbone.features.children())\n        if output_stride == 16:\n            self.low_level = nn.Sequential(*features[:3])\n            self.high_level = nn.Sequential(*features[3:])\n        else:\n            self.low_level = nn.Sequential(*features[:2])\n            self.high_level = nn.Sequential(*features[2:])\n\n        low_level_channels = 24 if output_stride == 16 else 16\n        self.low_proj = nn.Sequential(\n            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n            nn.BatchNorm2d(48),\n            nn.ReLU(inplace=True)\n        )\n\n        atrous_rates = [6, 12, 18] if output_stride == 16 else [12, 24, 36]\n        self.aspp = ASPP(1280, 256, atrous_rates)\n        self.mff = MFFBlock(48, 256, 256)\n        self.decoder = Decoder(256, 256)\n        self.cafse = CAFSEBlock(256)\n        self.final_conv = nn.Conv2d(256, num_classes_seg, 1)\n\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes_cls)\n        )\n\n        if activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        elif activation == 'softmax2d':\n            self.activation = nn.Softmax2d()\n        else:\n            self.activation = None\n\n    def forward(self, x):\n        input_size = x.size()[2:]\n        low_feat = self.low_level(x)\n        high_feat = self.high_level(low_feat)\n        low_proj = self.low_proj(low_feat)\n\n        if self.mode == 0:\n            out = self.classifier(high_feat)\n            return out\n        elif self.mode == 1:\n            aspp_out = self.aspp(high_feat)\n            mff_out = self.mff(low_proj, aspp_out)\n            decoder_out = self.decoder(mff_out, low_proj)\n            cafse_out = self.cafse(decoder_out, aspp_out)\n            out = self.final_conv(cafse_out)\n            out = F.interpolate(out, size=input_size, mode='bilinear', align_corners=True)\n            if self.activation is not None:\n                out = self.activation(out)\n            return out\n        else:\n            raise ValueError(\"Mode must be 0 (classification) or 1 (segmentation)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Compile","metadata":{}},{"cell_type":"code","source":"model = DeepFusionLab(num_classes_seg=2, num_classes_cls=2, mode=1)  # mode=1 for segmentation\n\n# Input image\ninput_tensor = torch.randn(2, 3, 256, 256)\n\n# Forward pass\noutput = model(input_tensor)\nprint(output.shape)\n\ndef print_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    non_trainable_params = total_params - trainable_params\n\n    print(f\"Total Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\")\n    print(f\"Non-Trainable Parameters: {non_trainable_params:,}\")\n\n# Example usage\nmodel = DeepFusionLab(num_classes_seg=2, num_classes_cls=2, mode=1)\nprint_model_parameters(model)\n\nsummary(model, input_size=(2, 3, 256, 256))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyK6CgXNzIvb","outputId":"151c2c90-b80b-4cde-f8c3-db66842d8e19","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics Define & Others","metadata":{}},{"cell_type":"code","source":"#Metrics definitions (ensure these are defined earlier in your code)\nclass DiceCoefficient(torch.nn.Module):\n    def __init__(self, threshold=0.5):\n        super(DiceCoefficient, self).__init__()\n        self.threshold = threshold\n        self.__name__ = \"DiceCoefficient\"  # Add the __name__ attribute\n\n    def forward(self, y_true, y_pred):\n        y_pred = torch.sigmoid(y_pred)  # Apply sigmoid if predictions are logits\n        y_pred = (y_pred > self.threshold).float()  # Apply threshold\n\n        intersection = (y_true * y_pred).sum(dim=(2, 3))  # Sum over height and width\n        union = (y_true + y_pred).sum(dim=(2, 3))\n\n        dice = 2. * intersection / (union + 1e-6)  # Add epsilon to avoid division by zero\n        return dice.mean()  # Mean over the batch\n\n\n\nclass IoU(torch.nn.Module):\n    def __init__(self, threshold=0.5, eps=1e-7, activation=None):\n        \"\"\"\n        Intersection over Union (IoU) metric similar to SMP's implementation.\n\n        Args:\n            threshold (float): Threshold for converting probabilities to binary predictions.\n            eps (float): Small value to avoid division by zero.\n            activation (callable, optional): Activation function to apply to predictions (e.g., torch.sigmoid).\n                                             If None, assumes inputs are already probabilities.\n        \"\"\"\n        super(IoU, self).__init__()\n        self.threshold = threshold\n        self.eps = eps\n        self.activation = activation\n        self.__name__ = \"IoU\"  # For compatibility with metric logging\n\n    def forward(self, y_pred, y_true):\n\n        # Apply activation if provided (e.g., sigmoid for logits)\n        if self.activation is not None:\n            y_pred = self.activation(y_pred)\n\n        # Convert probabilities to binary predictions using threshold\n        y_pred = (y_pred > self.threshold).float()\n\n        # Ensure inputs are binary and have matching shapes\n        y_true = y_true.float()\n        assert y_pred.shape == y_true.shape, f\"Shape mismatch: y_pred {y_pred.shape}, y_true {y_true.shape}\"\n\n        # Compute intersection and union\n        intersection = (y_pred * y_true).sum(dim=(2, 3))  # Sum over H and W dimensions\n        union = (y_pred + y_true - y_pred * y_true).sum(dim=(2, 3))  # Union = A + B - Aâˆ©B\n\n        # Compute IoU with epsilon to avoid division by zero\n        iou = (intersection + self.eps) / (union + self.eps)\n\n        # Return mean IoU over the batch\n        return iou.mean()\n\nclass AUC(torch.nn.Module):\n    def __init__(self):\n        super(AUC, self).__init__()\n        self.__name__ = \"AUC\"\n\n    def forward(self, y_pred, y_true):\n        # For a proper AUC, you would accumulate predictions and labels for the whole dataset\n        # Here we use a rough approximation per batch by binarizing with a threshold\n        y_pred = torch.sigmoid(y_pred).view(-1)\n        y_true = y_true.view(-1).float()\n        y_pred = y_pred.detach().cpu().numpy()\n        y_true = y_true.detach().cpu().numpy()\n\n        from sklearn.metrics import roc_auc_score\n        try:\n            auc = roc_auc_score(y_true, y_pred)\n        except ValueError:\n            auc = 0.5  # Fallback if only one class is present\n        return torch.tensor(auc)\n\nclass Accuracy(torch.nn.Module):\n    def __init__(self, threshold=0.5):\n        super(Accuracy, self).__init__()\n        self.threshold = threshold\n        self.__name__ = \"Accuracy\"  # âœ… Add this\n\n    def forward(self, y_pred, y_true):\n        # y_pred is already sigmoid activated\n        y_pred = (y_pred > self.threshold).float()\n        y_true = y_true.float()\n        correct = (y_pred == y_true).float()\n        return correct.mean()\n\n\nclass Precision(torch.nn.Module):\n    def __init__(self, threshold=0.5, eps=1e-7):\n        super(Precision, self).__init__()\n        self.threshold = threshold\n        self.eps = eps\n        self.__name__ = \"Precision\"\n\n    def forward(self, y_pred, y_true):\n        y_pred = (y_pred > self.threshold).float()\n        y_true = y_true.float()\n\n        TP = (y_pred * y_true).sum(dim=(2, 3))\n        FP = (y_pred * (1 - y_true)).sum(dim=(2, 3))\n\n        precision = (TP + self.eps) / (TP + FP + self.eps)\n        return precision.mean()\n\n\nclass Recall(torch.nn.Module):\n    def __init__(self, threshold=0.5, eps=1e-7):\n        super(Recall, self).__init__()\n        self.threshold = threshold\n        self.eps = eps\n        self.__name__ = \"Recall\"\n\n    def forward(self, y_pred, y_true):\n        y_pred = (y_pred > self.threshold).float()\n        y_true = y_true.float()\n\n        TP = (y_pred * y_true).sum(dim=(2, 3))\n        FN = ((1 - y_pred) * y_true).sum(dim=(2, 3))\n\n        recall = (TP + self.eps) / (TP + FN + self.eps)\n        return recall.mean()\n\n\nclass F1Score(torch.nn.Module):\n    def __init__(self, threshold=0.5, eps=1e-7):\n        super(F1Score, self).__init__()\n        self.threshold = threshold\n        self.eps = eps\n        self.__name__ = \"F1Score\"\n\n    def forward(self, y_pred, y_true):\n        y_pred = (y_pred > self.threshold).float()\n        y_true = y_true.float()\n\n        TP = (y_pred * y_true).sum(dim=(2, 3))\n        FP = (y_pred * (1 - y_true)).sum(dim=(2, 3))\n        FN = ((1 - y_pred) * y_true).sum(dim=(2, 3))\n\n        precision = (TP + self.eps) / (TP + FP + self.eps)\n        recall = (TP + self.eps) / (TP + FN + self.eps)\n        f1 = 2 * precision * recall / (precision + recall + self.eps)\n        return f1.mean()\n\ndice_metric = DiceCoefficient(threshold=0.5)\niou_metric = IoU(threshold=0.5)\nauc_metric = AUC()","metadata":{"id":"njvDMwJw3Xhf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ThresholdedDiceLoss(torch.nn.Module):\n    def __init__(self, threshold=0.5):\n        super(ThresholdedDiceLoss, self).__init__()\n        self.threshold = threshold\n        self.__name__ = 'dice_loss'  # Add the __name__ attribute\n\n    def forward(self, y_true, y_pred):\n        # Apply sigmoid if the predictions are logits (before thresholding)\n        y_pred = torch.sigmoid(y_pred)\n\n        # Apply thresholding to the predicted probabilities\n        y_pred = (y_pred > self.threshold).float()\n\n        # Calculate intersection and union\n        intersection = (y_true * y_pred).sum(dim=(2, 3))  # Sum over height and width\n        union = (y_true + y_pred).sum(dim=(2, 3))\n\n        # Calculate Dice coefficient and return the loss (1 - Dice coefficient)\n        dice = 2. * intersection / (union + 1e-6)  # Add epsilon to avoid division by zero\n        return 1 - dice.mean()  # Loss is 1 - Dice coefficient, averaged over the batch\n\n\nloss_fn = ThresholdedDiceLoss(threshold=0.5)\n\n# Early Stopping class\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=1e-20, metric='val_iou'):\n        self.patience = patience  # Number of epochs to wait for improvement\n        self.min_delta = min_delta  # Minimum improvement required\n        self.metric = metric  # Metric to monitor ('val_iou', 'val_dice', or 'val_loss')\n        self.best_score = None\n        self.wait = 0\n        self.stop_training = False\n\n    def __call__(self, metrics_dict):\n        current_score = metrics_dict[self.metric]\n        if self.metric == 'val_loss':\n            current_score = -current_score  # Lower loss is better, so negate for consistency\n\n        if self.best_score is None:\n            self.best_score = current_score\n        elif current_score < self.best_score + self.min_delta:\n            self.wait += 1\n            print(f\"No improvement in {self.metric}. Wait: {self.wait}/{self.patience}\")\n            if self.wait >= self.patience:\n                self.stop_training = True\n                print(f\"Early stopping triggered after {self.wait} epochs without improvement!\")\n        else:\n            self.best_score = current_score\n            self.wait = 0\n\n# One-hot encoding and decoding\ndef one_hot_encode(label, label_values):\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis=-1)\n        semantic_map.append(class_map)\n    return np.stack(semantic_map, axis=-1)\n\ndef reverse_one_hot(image):\n    return np.argmax(image, axis=-1)\n\ndef colour_code_segmentation(image, label_values):\n    colour_codes = np.array(label_values)\n    return colour_codes[image.astype(int)]","metadata":{"id":"5vCw0Hdk3421","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the threshold\nthreshold = np.array([112, 127, 127])\n\n# Define the input directory for masks\nmask_dir = '/kaggle/working/Resized/Train/Mask'\n\n# List all files in the mask directory\nmask_files = os.listdir(mask_dir)\n\n# Iterate through each mask file\nfor mask_filename in mask_files:\n    mask_path = os.path.join(mask_dir, mask_filename)\n\n    # Read the image\n    mask_img = cv2.imread(mask_path)\n\n    if mask_img is None:\n        print(f\"Could not read image: {mask_path}\")\n        continue\n\n    # Check if the image is grayscale or color and convert to BGR if needed for comparison\n    if len(mask_img.shape) == 2: # Grayscale\n        # Convert grayscale to 3 channels to compare with a 3-channel threshold\n        mask_img_color = cv2.cvtColor(mask_img, cv2.COLOR_GRAY2BGR)\n    else: # Color image\n        mask_img_color = mask_img.copy() # Work on a copy\n\n    # Create a boolean mask where pixels are less than the threshold in all channels\n    # Note: cv2.threshold and similar functions are typically for single-channel images.\n    # We can use numpy broadcasting and boolean indexing for multi-channel thresholding.\n    mask_below_threshold = np.all(mask_img_color < threshold, axis=-1)\n\n    # Create the output image (initialized to black)\n    thresholded_mask = np.zeros_like(mask_img_color, dtype=np.uint8)\n\n    # Set pixels greater than or equal to the threshold to [255, 255, 255] (white)\n    # The condition is the opposite of mask_below_threshold\n    thresholded_mask[~mask_below_threshold] = [255, 255, 255]\n\n    # Save the thresholded image back to the same location (overwriting the original mask)\n    cv2.imwrite(mask_path, thresholded_mask)\n\nprint(\"Mask images on Train/Masks have been thresholded.\")","metadata":{"id":"8y68mSHZANbf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d80a9676-365b-49b4-ef02-550d464bba9a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the input directory for masks\nmask_dir = '/kaggle/working/Resized/Test/Mask'\n\n# List all files in the mask directory\nmask_files = os.listdir(mask_dir)\n\n# Iterate through each mask file\nfor mask_filename in mask_files:\n    mask_path = os.path.join(mask_dir, mask_filename)\n\n    # Read the image\n    mask_img = cv2.imread(mask_path)\n\n    if mask_img is None:\n        print(f\"Could not read image: {mask_path}\")\n        continue\n\n    # Check if the image is grayscale or color and convert to BGR if needed for comparison\n    if len(mask_img.shape) == 2: # Grayscale\n        # Convert grayscale to 3 channels to compare with a 3-channel threshold\n        mask_img_color = cv2.cvtColor(mask_img, cv2.COLOR_GRAY2BGR)\n    else: # Color image\n        mask_img_color = mask_img.copy() # Work on a copy\n\n    # Create a boolean mask where pixels are less than the threshold in all channels\n    mask_below_threshold = np.all(mask_img_color < threshold, axis=-1)\n\n    # Create the output image (initialized to black)\n    thresholded_mask = np.zeros_like(mask_img_color, dtype=np.uint8)\n\n    # Set pixels greater than or equal to the threshold to [255, 255, 255] (white)\n    # The condition is the opposite of mask_below_threshold\n    thresholded_mask[~mask_below_threshold] = [255, 255, 255]\n\n    # Save the thresholded image back to the same location (overwriting the original mask)\n    cv2.imwrite(mask_path, thresholded_mask)\n\nprint(\"Mask images on Test/Masks have been thresholded.\")\n","metadata":{"id":"T25hxW6fBziM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3bf1d3fc-841d-46a9-9055-ceabd43c64f7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the base directory for the dataset\ndataset_base_dir = '/kaggle/working/Resized'\ntrain_dir = os.path.join(dataset_base_dir, 'Train')\ntest_dir = os.path.join(dataset_base_dir, 'Test')\n\n# List image files in the Train directory\ntrain_image_files = [f for f in os.listdir(os.path.join(train_dir, 'Image')) if f.endswith('.png')]\n\n# Create a list of full paths for the training images and masks\ntrain_image_paths = [os.path.join(train_dir, 'Image', f) for f in train_image_files]\ntrain_mask_paths = [os.path.join(train_dir, 'Mask', f) for f in train_image_files] # Assuming mask filenames match image filenames\n\n# Create a DataFrame for the training data\ntrain_df_full = pd.DataFrame({'image_path': train_image_paths, 'mask_path': train_mask_paths})\n\n# Split the full training data into training and validation sets\ntrain_df, valid_df = train_test_split(train_df_full, test_size=0.1, random_state=42) # 90% train, 10% validation\n\n# Reset indices after splitting\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\n\n# List image files in the Test directory\ntest_image_files = [f for f in os.listdir(os.path.join(test_dir, 'Image')) if f.endswith('.png')]\n\n# Create a list of full paths for the test images and masks\ntest_image_paths = [os.path.join(test_dir, 'Image', f) for f in test_image_files]\ntest_mask_paths = [os.path.join(test_dir, 'Mask', f) for f in test_image_files] # Assuming mask filenames match image filenames\n\n# Create a DataFrame for the test data\ntest_df = pd.DataFrame({'image_path': test_image_paths, 'mask_path': test_mask_paths})\n\nprint(f\"Number of training samples: {len(train_df)}\")\nprint(f\"Number of validation samples: {len(valid_df)}\")\nprint(f\"Number of test samples: {len(test_df)}\")\n\n# Display the first few rows of each DataFrame\nprint(\"\\nTrain DataFrame:\")\nprint(train_df.head())\nprint(\"\\nValidation DataFrame:\")\nprint(valid_df.head())\nprint(\"\\nTest DataFrame:\")\nprint(test_df.head())\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sQcJM286saZ","outputId":"1d3b315f-09b5-4c22-e677-e2983f83ae78","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyDataGenerator(Dataset):\n    def __init__(self, df, class_rgb_values, augmentation=None, preprocessing=None):\n        self.image_paths = df['image_path'].tolist()\n        self.mask_paths = df['mask_path'].tolist()\n        self.class_rgb_values = class_rgb_values\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n\n    def __getitem__(self, i):\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n\n        # One-hot encode to shape (H, W, C), values in {0, 1}\n        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')  # âœ… no division\n\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        return image, mask\n\n    def __len__(self):\n        return len(self.image_paths)\n\n\n# Augmentations\ndef get_training_augmentation():\n    return album.Compose([\n        album.HorizontalFlip(p=0.5),  # Existing: Randomly flip horizontally\n        album.VerticalFlip(p=0.5),    # Randomly flip vertically (polyps can appear in any orientation)\n    ])\n\ndef get_validation_augmentation():\n    return album.Compose([\n        album.PadIfNeeded(min_height=256, min_width=256, always_apply=True, border_mode=cv2.BORDER_CONSTANT, value=0)\n    ])\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing():\n    return album.Compose([\n        album.Resize(height=256, width=256, always_apply=True),  # Resize to 256x256\n        album.Lambda(image=to_tensor, mask=to_tensor)  # Convert to tensor\n    ])\n\nselect_class_rgb_values = np.array([[0, 0, 0],\n                            [255, 255, 255]])","metadata":{"id":"gg96EaYl7ktj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize data loaders\ntrain_dataset = MyDataGenerator(train_df, select_class_rgb_values, get_training_augmentation(), get_preprocessing())\nvalid_dataset = MyDataGenerator(valid_df, select_class_rgb_values, get_validation_augmentation(), get_preprocessing())\ntest_dataset = MyDataGenerator(test_df, select_class_rgb_values, get_validation_augmentation(), get_preprocessing())\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Initialize model, loss, optimizer, and scheduler\nmodel = DeepFusionLab(num_classes_seg=2, num_classes_cls=2, mode=1).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=1e-9)\n\n# Metric storage lists\ntrain_loss_list, valid_loss_list = [], []\ntrain_dice_list, valid_dice_list = [], []\ntrain_iou_list, valid_iou_list = [], []\n\n# Training loop\nbest_iou = 0.0\nearly_stopping = EarlyStopping(patience=pat, min_delta=1e-20, metric='val_iou')\n# Start timing\nstart_time = time.time()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss, train_dice, train_iou = 0.0, 0.0, 0.0\n\n    for images, masks in train_loader:\n        images, masks = images.to(DEVICE), masks.to(DEVICE)\n        optimizer.zero_grad()\n        preds = model(images)\n        loss = loss_fn(preds, masks)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * images.size(0)\n        train_dice += dice_metric(preds, masks).item() * images.size(0)\n        train_iou += iou_metric(preds, masks).item() * images.size(0)\n\n    train_loss /= len(train_loader.dataset)\n    train_dice /= len(train_loader.dataset)\n    train_iou /= len(train_loader.dataset)\n\n    model.eval()\n    valid_loss, valid_dice, valid_iou = 0.0, 0.0, 0.0\n\n    with torch.no_grad():\n        for images, masks in valid_loader:\n            images, masks = images.to(DEVICE), masks.to(DEVICE)\n            preds = model(images)\n            loss = loss_fn(preds, masks)\n\n            valid_loss += loss.item() * images.size(0)\n            valid_dice += dice_metric(preds, masks).item() * images.size(0)\n            valid_iou += iou_metric(preds, masks).item() * images.size(0)\n\n    valid_loss /= len(valid_loader.dataset)\n    valid_dice /= len(valid_loader.dataset)\n    valid_iou /= len(valid_loader.dataset)\n\n    # Logging\n    print(f\"Epoch {epoch+1}: \"\n          f\"Train Loss={train_loss:.4f}, Train Dice={train_dice:.4f}, Train IoU={train_iou:.4f}, \"\n          f\"Valid Loss={valid_loss:.4f}, Valid Dice={valid_dice:.4f}, Valid IoU={valid_iou:.4f}\")\n\n    # Store metrics\n    train_loss_list.append(train_loss)\n    valid_loss_list.append(valid_loss)\n\n    train_dice_list.append(train_dice)\n    valid_dice_list.append(valid_dice)\n\n    train_iou_list.append(train_iou)\n    valid_iou_list.append(valid_iou)\n\n    # Save best model\n    if valid_iou > best_iou:\n        best_iou = valid_iou\n        torch.save(model.state_dict(), 'Best_Weight.pth')\n        print(\"Model saved!\")\n\n    # Early stopping\n    metrics_dict = {'val_loss': valid_loss, 'val_dice': valid_dice, 'val_iou': valid_iou}\n    early_stopping(metrics_dict)\n    if early_stopping.stop_training:\n        print(f\"Training stopped at epoch {epoch+1}\")\n        break\n\n    scheduler.step()\n\n# End timing\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"\\nTotal training time: {elapsed_time / 60:.2f} minutes\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZ3lMlxJ4Xqu","outputId":"f42d9704-0b45-4a7e-c91f-6e3a5ae19921","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting\nepochs_range = range(1, len(train_loss_list) + 1)\nplt.figure(figsize=(18, 5))\n\n# Loss plot\nplt.subplot(1, 3, 1)\nplt.plot(epochs_range, train_loss_list, label='Train Loss')\nplt.plot(epochs_range, valid_loss_list, label='Val Loss')\nplt.title('Loss over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Dice score plot\nplt.subplot(1, 3, 2)\nplt.plot(epochs_range, train_dice_list, label='Train Dice')\nplt.plot(epochs_range, valid_dice_list, label='Val Dice')\nplt.title('Dice Score over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Dice Score')\nplt.legend()\n\n# IoU plot\nplt.subplot(1, 3, 3)\nplt.plot(epochs_range, train_iou_list, label='Train IoU')\nplt.plot(epochs_range, valid_iou_list, label='Val IoU')\nplt.title('IoU over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\nmodel = DeepFusionLab(num_classes_seg=2, num_classes_cls=2, mode=1).to(DEVICE)\n\n# Load best weights\nmodel.load_state_dict(torch.load('Best_Weight.pth', map_location=DEVICE))\nprint(\"âœ… Loaded best model weights\")\n\n# Set to evaluation mode\nmodel.eval()\n\n# Create test loader\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\n# Metrics\nmetrics = [\n    DiceCoefficient(threshold=0.5),\n    IoU(threshold=0.5),\n    AUC(),\n    Accuracy(threshold=0.5),\n    Precision(threshold=0.5),\n    Recall(threshold=0.5),\n    F1Score(threshold=0.5),\n]\n\n# Initialize accumulators\ntest_stats = {metric.__name__: 0.0 for metric in metrics}\ntest_loss = 0.0\ntotal_samples = 0\n\n# Evaluation loop\nwith torch.no_grad():\n    for images, masks in test_loader:\n        images, masks = images.to(DEVICE), masks.to(DEVICE)\n        preds = model(images)\n\n        # Loss\n        loss = loss_fn(preds, masks)\n        test_loss += loss.item() * images.size(0)\n\n        # Metrics\n        for metric in metrics:\n            test_stats[metric.__name__] += metric(preds, masks).item() * images.size(0)\n\n        total_samples += images.size(0)\n\n# Average everything\ntest_loss /= total_samples\nfor key in test_stats:\n    test_stats[key] /= total_samples\n\n# Print results\nprint(\"\\nðŸ“Š Test Evaluation Results:\")\nprint(f\"  Loss: {test_loss:.4f}\")\nfor key, value in test_stats.items():\n    print(f\"  {key}: {value:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}